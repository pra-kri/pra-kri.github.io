---
layout: page
title: T.E.001 - Entropy
---

17/02/20

### Technical Essay 001 : On Entropy


##### 0. WHat is the Shannon Information Content of the outcome of a r.v.? 
* The less probable an outcome is, the more Shannon information content the outcome has.
* Why is it log(1/p)? Additivity property for independent r.v.s. (Shannon info of X + Shannon info of Y)
* 
##### 1. What is Entropy?
* Average of Shannon Information Content of a random variable

##### 2. What is it used for?
##### 3. 
